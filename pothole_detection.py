# -*- coding: utf-8 -*-
"""Pothole_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-JKdPxNUHQkr30iJfNM1r9mPxIHT5ffq

# Generate ssh key for auto-annotation
"""

!rm -r sample_data/

!ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ''

!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts

!cat ~/.ssh/id_rsa.pub

!ssh -T git@github.com

# Commented out IPython magic to ensure Python compatibility.
# clone repository and navigate to root directory
!git clone git@github.com:roboflow-ai/auto-annotate.git
# %cd auto-annotate

!apt-get install python3-venv

!python3 -m venv venv
!source venv/bin/activate

# install
!pip install -e .

!pip -q install roboflow fastdup ultralytics

from ultralytics import YOLO
import roboflow
from roboflow import Roboflow
from IPython.display import IFrame
import glob
import os
import fastdup

#if directory is here
!rm -r pothole-cleaned-10/
!rm *.zip
!rm -r sample_data

"""# Login into Roboflow"""

roboflow.login()
rf = Roboflow()

#Download classification project
project = rf.workspace("pothole-classification").project("pothole-classification-lsygd")
print(project)

#for version number, you need to create a new version in roboflow "Versions" tab
dataset = project.version(6).download("folder")

#Download object detection dataset
project = rf.workspace("pothole-h33io").project("pothole-cleaned")
print(project)
dataset = project.version(14).download("yolov8")

"""# Adding new data"""

from google.colab import files
uploaded = files.upload()

#if images dir doesn't exist, create it
!mkdir images/

#Move all image file types to images directory
!mv *.jpg images/
!mv *.webp images/
!mv *.png images/
!mv *.jfif images/

#check # of imgs in images folder
!ls images -1A | grep -v '^(\.\.|\.)$' | wc -l

!ls images/

import os
dir = "images/"
#simple adding all files from images directory into project
print(project)
for file in os.listdir(dir):
  project.upload(dir + file)
  print(file)

#Remove all jpg images from images directory and all json annotation files from labels directory
!rm images/*.jpg
!rm labels/*.json

from PIL import Image
import os

#deletes existing jfif and webp files if their corresponding jpg file exists
def delete_old_jfif_webp_files(directory):
    for pwd, _, files in os.walk(directory):
        for file in files:
            if file.lower().endswith(('.jfif', '.webp')):
              file_path = os.path.join(pwd, file)
              if os.path.exists(file_path):
                os.remove(file_path)
                print(f"Deleted: {file}")


#converts jfif and webp to jpg
def convert_images(image_dir):
    for root, _, files in os.walk(image_dir):
        for file in files:
            if file.lower().endswith(('.jfif', '.webp')):
                file_path = os.path.join(root, file)
                try:
                    img = Image.open(file_path)
                    new_filename = os.path.splitext(file)[0] + ".jpg"
                    new_file_path = os.path.join(root, new_filename)
                    img.save(new_file_path)
                    print(f"Converted {file_path} to {new_file_path}")
                    # Optionally delete the original file after conversion:
                    # os.remove(file_path)
                except Exception as e:
                    print(f"Error converting {file_path}: {e}")

image_directory = "images"
convert_images(image_directory)
delete_old_jfif_webp_files(image_directory)

# Commented out IPython magic to ensure Python compatibility.
#auto-annotates images in images directory
# %env ROBOFLOW_API_KEY=CWhevHNkKVaAMb39zuOy
!python -m a2.annotate \
--source_image_directory images \
--target_annotation_directory labels \
--roboflow_project_id pothole-detection-rzpei \
--roboflow_project_version 1

import imghdr
import os

#checks types of image file in dataset
def get_image_type(filename):
  with open(filename, 'rb') as f:
    return imghdr.what(f)

for file in os.listdir("images"):
  file_path = os.path.join("images", file)
  image_type = get_image_type(file_path)
  print(f"The image type of {file_path} is {image_type}")

import glob
import os
from roboflow import Roboflow
import json

img_dir = "images/"
label_dir = "labels/"
img_file_extension = ".jpg"
label_file_extension = ".json"

# Upload images
image_glob = glob.glob(img_dir + '/*' + img_file_extension)
image_glob.sort()

# Upload images with and without annotations
#annotations are stored in json format, if predictions is empty, uploading json file will cause single_upload() to fail
#Does not handle if img is already annotate
for image_path in image_glob:
    annotation_path = label_dir + os.path.splitext(os.path.basename(image_path))[0] + label_file_extension
    if os.path.exists(annotation_path):
        # Check if annotation file has predictions
        with open(annotation_path, 'r') as f:
            annotation_data = json.load(f)
        if annotation_data.get("predictions"):  # Check if 'predictions' key exists and is not empty
            # Upload with annotation
            print(f"{image_path} has prediction",project.single_upload(
                image_path=image_path,
                annotation_path=annotation_path,
                split="valid",
            ))
        else:
            print(f"{image_path} has empty predictions. Will upload normally.")
            project.upload(image_path=image_path, split="test")
    else:
      print("No annotation file")

"""# Classification"""

#deletes existing fastdup objects
!rm -r test_not/
!rm -r test_pot/
!rm -r train_not/
!rm -r train_pot/
!rm -r valid_not/
!rm -r valid_pot/

fd_train_pot = fastdup.create(work_dir="./train_pot/", input_dir=dataset.location + '/train/Pothole/')
fd_train_pot.run()
fd_train_not = fastdup.create(work_dir="./train_not/", input_dir=dataset.location + '/train/Not Pothole/')
fd_train_not.run()

fd_valid_pot = fastdup.create(work_dir="./valid_pot/", input_dir=dataset.location + '/valid/Pothole/')
fd_valid_pot.run()
fd_valid_not = fastdup.create(work_dir="./valid_not/", input_dir=dataset.location + '/valid/Not Pothole/')
fd_valid_not.run()

fd_test_pot = fastdup.create(work_dir="./test_pot/", input_dir=dataset.location + '/test/Pothole/')
fd_test_pot.run()
fd_test_not = fastdup.create(work_dir="./test_not/", input_dir=dataset.location + '/test/Not Pothole/')
fd_test_not.run()

fd_train_pot.summary()
fd_train_not.summary()
fd_valid_pot.summary()
fd_valid_not.summary()
fd_test_pot.summary()
fd_test_not.summary()

fd_train_pot.vis.duplicates_gallery()
# fd_train_not.vis.duplicates_gallery()
fd_valid_pot.vis.duplicates_gallery()
# fd_valid_not.vis.duplicates_gallery()
fd_test_pot.vis.duplicates_gallery()
# fd_test_not.vis.duplicates_gallery()

fd_train_pot.vis.stats_gallery(metric='blur')
fd_train_not.vis.stats_gallery(metric='blur')
fd_valid_pot.vis.stats_gallery(metric='blur')
fd_valid_not.vis.stats_gallery(metric='blur')
fd_test_pot.vis.stats_gallery(metric='blur')
fd_test_not.vis.stats_gallery(metric='blur')

"""# Object detection"""

#run if these directories already exist and want to test on new dataset
!rm -r train/
!rm -r test/
!rm -r valid/

fd_train = fastdup.create(work_dir="./train/", input_dir=dataset.location + '/train/')
fd_train.run()

fd_valid = fastdup.create(work_dir="./valid/", input_dir=dataset.location + '/valid/')
fd_valid.run()

fd_test = fastdup.create(work_dir="./test/", input_dir=dataset.location + '/test/')
fd_test.run()

fd_train.summary()
fd_valid.summary()
fd_test.summary()

fd_train.vis.stats_gallery(metric='blur')
fd_valid.vis.stats_gallery(metric='blur')
fd_test.vis.stats_gallery(metric='blur')

fd_train.vis.duplicates_gallery()
fd_valid.vis.duplicates_gallery()
fd_test.vis.duplicates_gallery()

from google.colab.patches import cv2_imshow
import cv2
stats_df = fd_train.img_stats()
filtered_df = stats_df[(stats_df['blur'] >= 50) & (stats_df['blur'] <= 500)].sort_values('blur')
filtered_df.head()
for index, row in filtered_df.iterrows():
    filename = row['filename']
    blur_score = row['blur']
    try:
        img = cv2.imread(filename)
        text = f"Blur: {blur_score:.2f}"
        font = cv2.FONT_HERSHEY_SIMPLEX
        textsize = cv2.getTextSize(text, font, 1, 2)[0]
        textX = (img.shape[1] - textsize[0]) // 2
        textY = img.shape[0] - 20  # Position text near the bottom
        cv2.putText(img, text, (textX, textY), font, 1, (255, 255, 255), 2)  # White text
        cv2_imshow(img)
        cv2.waitKey(0)
        cv2.destroyAllWindows()
    except Exception as e:
        print(f"Error with {filename}: {e}")

import os
import cv2
bad_list=[]
dir=r'pothole-cleaned-14/'
subdir_list=os.listdir(dir) # create a list of the sub directories in the directory ie train or test
for d in subdir_list:  # iterate through the sub directories train and test
    dpath=os.path.join (dir, d) # create path to sub directory
    if d in ['test/images', 'train/images', 'valid/images']:
      class_list=os.listdir(dpath) # list of classes ie dog or cat
      print (class_list)
      for klass in class_list: # iterate through the two classes
        class_path=os.path.join(dpath, klass) # path to class directory
        print(class_path)
        file_list=os.listdir(class_path) # create list of files in class directory
        for f in file_list: # iterate through the files
            fpath=os.path.join (class_path,f)
            index=f.rfind('.') # find index of period infilename
            ext=f[index+1:] # get the files extension
            if ext  not in ['jpg', 'png', 'bmp', 'gif']:
                print(f'file {fpath}  has an invalid extension {ext}')
                bad_list.append(fpath)
            else:
                try:
                    img=cv2.imread(fpath)
                    size=img.shape
                except:
                    print(f'file {fpath} is not a valid image file ')
                    bad_list.append(fpath)

print (bad_list)

!rm images/*

import os
import shutil
from google.colab import files
uploaded = files.upload()
image_dir = 'images'
if not os.path.exists(image_dir):
    os.makedirs(image_dir)
for filename in uploaded.keys():
    src_path = filename
    dst_path = os.path.join(image_dir, filename)
    if os.path.exists(dst_path):
        os.remove(src_path) #Delete the uploaded image
        print(f"Deleted uploaded file as duplicate: {src_path}")
    else:
        shutil.move(src_path, dst_path)
        print(f"Moved {filename} to {image_dir}")

!rm images/*

!find images -maxdepth 1 -type f | wc -l

!touch names.txt

# Replace 'filenames.txt' with the actual path to your file
with open('names.txt', 'r') as f:
  filenames_txt = [line.strip().split('.')[0] for line in f]  # Extract filenames, ignore extensions

import os
image_dir = 'images'
filenames_images = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]
# Extract filenames without extensions
filenames_images_noext = [os.path.splitext(f)[0] for f in filenames_images]

all_exist = all(filename in filenames_images_noext for filename in filenames_txt)
if all_exist:
  print("All filenames in the text file exist in the 'images' directory.")
else:
  missing_files = [filename for filename in filenames_txt if filename not in filenames_images_noext]
  print("The following filenames are missing in the 'images' directory:")
  for filename in missing_files:
    print(filename)

from PIL import Image
import os

def convert_jfif_to_jpg(image_dir):
    for root, _, files in os.walk(image_dir):
        for file in files:
            if file.lower().endswith('.jfif'):
                file_path = os.path.join(root, file)
                try:
                    img = Image.open(file_path)
                    new_filename = os.path.splitext(file)[0] + ".jpg"
                    new_file_path = os.path.join(root, new_filename)
                    img.save(new_file_path)
                    print(f"Converted {file_path} to {new_file_path}")
                    os.remove(file_path) # Delete original JFIF file
                except Exception as e:
                    print(f"Error converting {file_path}: {e}")

# Call the function with the directory containing your images
image_directory = "images"  # Replace with your actual image directory
convert_jfif_to_jpg(image_directory)

import os

def remove_jfif_duplicates(image_dir):
    for filename in os.listdir(image_dir):
        if filename.lower().endswith('.jfif'):
            jpg_filename = os.path.splitext(filename)[0] + ".jpg"
            jpg_filepath = os.path.join(image_dir, jpg_filename)
            jfif_filepath = os.path.join(image_dir, filename)
            if os.path.exists(jpg_filepath):
                os.remove(jfif_filepath)
                print(f"Removed JFIF duplicate: {jfif_filepath}")

# Call the function to remove JFIF duplicates in the 'images' directory
image_directory = "images"
remove_jfif_duplicates(image_directory)

import matplotlib.pyplot as plt
# plt.hist(stats['blur'])
plt.hist(train_df['blur'], bins=range(0, int(train_df['blur'].max()) + 10, 10))  # Bins in steps of 20
plt.xlabel('Blur Score')
plt.ylabel('Frequency')
plt.xlim(0, 300)
plt.title('Blur Distribution')
plt.show()

"""# For object detection"""

fd_train = fastdup.create(work_dir="./train/", input_dir=dataset.location + '/train/images')
fd_train.run()
fd_test = fastdup.create(work_dir="./test/", input_dir=dataset.location + '/test/images')
fd_test.run()
fd_valid = fastdup.create(work_dir="./valid/", input_dir=dataset.location + '/valid/images')
fd_valid.run()

fd_train.summary()

fd_valid.summary()

fd_test.summary()

fd_train.invalid_instances()

fd_valid.invalid_instances()

fd_test.invalid_instances()

"""# Duplicates"""

fd_train.vis.duplicates_gallery()

fd_valid.vis.duplicates_gallery()

fd_test.vis.duplicates_gallery()

connected_components_df_train , _ = fd_train.connected_components()
connected_components_df_test , _ = fd_test.connected_components()
connected_components_df_valid , _ = fd_valid.connected_components()
connected_components_df_train.head()

connected_components_df_valid.head()

connected_components_df_test.head()

from collections import defaultdict

def count_duplicates_before_rf(filenames):
  prefix_counts = defaultdict(int)
  for filename in filenames:
    prefix = filename.split('.rf')[0]
    prefix_counts[prefix] += 1
  return prefix_counts

threshold = 0.999999  # Set your desired similarity threshold

# Filter the DataFrame based on mean distance
similar_images_df = connected_components_df_train[connected_components_df_train['mean_distance'] == threshold]
print("There are", len(similar_images_df['filename']), "duplicates")
prefix_counts = count_duplicates_before_rf(similar_images_df['filename'])

for file in similar_images_df['filename']:
  print(file)
print("Files with more than 2 duplicates (considering prefix before '.rf'):")
for prefix, count in prefix_counts.items():
  if count > 2:
    print(prefix)

# a function to group connected components
def get_clusters(df, sort_by='count', min_count=2, ascending=False):
    # columns to aggregate
    if df.empty:
        return df
    agg_dict = {'filename': list, 'mean_distance': max, 'count': len}

    if 'label' in df.columns:
        agg_dict['label'] = list

    # filter by count
    df = df[df['count'] >= min_count]

    # group and aggregate columns
    grouped_df = df.groupby('component_id').agg(agg_dict)

    # sort
    grouped_df = grouped_df.sort_values(by=[sort_by], ascending=ascending)
    return grouped_df

clusters_df_train = get_clusters(connected_components_df_train)
clusters_df_test = get_clusters(connected_components_df_test)
clusters_df_valid = get_clusters(connected_components_df_valid)

def process_clusters(clusters_df):
    cluster_images_to_keep = []
    list_of_duplicates = []
    if clusters_df.empty:
      return cluster_images_to_keep, list_of_duplicates

    for cluster_file_list in clusters_df.filename:
        # keep first file, discard rest
        keep = cluster_file_list[0]
        discard = cluster_file_list[1:]

        cluster_images_to_keep.append(keep)
        list_of_duplicates.extend(discard)

    print(f"Found {len(set(list_of_duplicates))} highly similar images to discard")
    return cluster_images_to_keep, list_of_duplicates

# Process clusters for each set
cluster_images_to_keep_train, list_of_duplicates_train = process_clusters(clusters_df_train)
cluster_images_to_keep_test, list_of_duplicates_test = process_clusters(clusters_df_test)
cluster_images_to_keep_valid, list_of_duplicates_valid = process_clusters(clusters_df_valid)

"""# Outliers

"""

fd_train.vis.outliers_gallery()
fd_test.vis.outliers_gallery()
fd_valid.vis.outliers_gallery()

outlier_df_train = fd_train.outliers()
list_of_outliers_train = outlier_df_train[outlier_df_train.distance < 0.68].filename_outlier.tolist()
outlier_df_test = fd_test.outliers()
list_of_outliers_test = outlier_df_test[outlier_df_test.distance < 0.68].filename_outlier.tolist()
outlier_df_valid = fd_valid.outliers()
list_of_outliers_valid = outlier_df_valid[outlier_df_valid.distance < 0.68].filename_outlier.tolist()

print(len(list_of_outliers_train))
print(len(list_of_outliers_test))
print(len(list_of_outliers_valid))

"""# Dark images"""

fd_train.vis.stats_gallery(metric='dark')

"""# Bright Images"""

fd_train.vis.stats_gallery(metric='bright')

"""# Blurry images"""

fd_train.vis.stats_gallery(metric='blur')
fd_test.vis.stats_gallery(metric='blur')
fd_valid.vis.stats_gallery(metric='blur')

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
train_df = fd_train.img_stats()
count = 0
filtered_df = train_df[(train_df['blur'] >= 50) & (train_df['blur'] <= 300)].sort_values('blur')  # Correct filtering

# Plotting
for index, row in filtered_df.iterrows():
    filename = row['filename']
    blur_score = row['blur']
    img = mpimg.imread(filename)
    count += 1
    plt.figure()
    plt.imshow(img)
    plt.title(f"{filename} - Blur: {blur_score:.2f}")
    plt.show()

print("There are ", count, " images")
  #blur of 50 and above is acceptable, but need to remove duplicates, imgs with rainy like augmentation, and partial
  #potholes

import pandas as pd
with pd.option_context('display.max_rows', None):
    print(filtered_df)

!cat ./pothole-cleaned-1/train/labels/923_jpg.rf.6c9a0e730f64ec39041afbaefdfe80ca.txt

import cv2
import numpy as np
import os

directory = 'pothole-cleaned-9/train/images'

def has_rain_like_pattern(image_path, high_freq_threshold=0.1):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    f = np.fft.fft2(img)
    fshift = np.fft.fftshift(f)
    magnitude_spectrum = 20*np.log(np.abs(fshift))

    # Calculate the proportion of high frequencies
    high_freq_ratio = np.sum(magnitude_spectrum[magnitude_spectrum > 150]) / np.sum(magnitude_spectrum)

    if high_freq_ratio > high_freq_threshold:
        return True
    else:
        return False

for filename in os.listdir(directory):
# image_to_check = "pothole-cleaned-1/train/images/923_jpg.rf.6c9a0e730f64ec39041afbaefdfe80ca.jpg"
  image_to_check = os.path.join(directory, filename) # Construct the full image path
  if has_rain_like_pattern(image_to_check):
      print(image_to_check,"Image likely contains rain-like patterns.")
  else:
      print("Image likely does not contain rain-like patterns.")

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

train_df = fd_train.img_stats()
filtered_df = train_df[train_df['blur'] > 50]

for index, row in filtered_df.iterrows():
    filename = row['filename']
    blur_score = row['blur']
    img = mpimg.imread(filename)
    plt.figure()
    plt.imshow(img)
    plt.title(f"{filename} - Blur: {blur_score:.2f}")
    plt.show()

stats_df_train = fd_train.img_stats()
list_of_blurry_images_train = stats_df_train[stats_df_train['blur'] < 50]['filename'].to_list()
stats_df_test = fd_test.img_stats()
list_of_blurry_images_test = stats_df_test[stats_df_test['blur'] < 50]['filename'].to_list()
stats_df_valid = fd_valid.img_stats()
list_of_blurry_images_valid = stats_df_valid[stats_df_valid['blur'] < 50]['filename'].to_list()

print(list_of_blurry_images_test)
print(list_of_blurry_images_valid)
print(list_of_blurry_images_train)

import matplotlib.pyplot as plt
# plt.hist(stats['blur'])
plt.hist(stats['blur'], bins=range(0, int(stats['blur'].max()) + 10, 10))  # Bins in steps of 20
plt.xlabel('Blur Score')
plt.ylabel('Frequency')
plt.xlim(0, 300)
plt.title('Blur Distribution')
plt.show()

import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

train_df = fd_train.img_stats()

# Set your blur threshold for deletion
blur_threshold = 50

# Get a list of filenames of images to delete
images_to_delete = train_df[train_df['blur'] <= blur_threshold]['filename'].tolist()

# Confirm before deleting (optional, but highly recommended)
if images_to_delete:
    print("Images to be deleted:")
    for filename in images_to_delete:
        print(filename)

    confirm = input("Are you sure you want to delete these images? (yes/no): ")
    if confirm.lower() == 'yes':
        # Delete the images
        for filename in images_to_delete:
            if os.path.exists(filename):
                os.remove(filename)
                print(f"Deleted: {filename}")
            else:
                print(f"File not found: {filename}")
    else:
        print("Deletion canceled.")
else:
    print("No images found with blur score below the threshold.")

"""# Deletion"""

def delete_images(list, dir_path):
    num_deleted = 0

    for file_path in list:
        try:
            os.remove(file_path)
            num_deleted += 1
        except Exception as e:
            print(f"Error occurred when deleting file {file_path}: {e}")

    print(f"Deleted {num_deleted} images")

    # Count the number of images left
    remaining_images = len(os.listdir(dir_path))
    print(f"There are {remaining_images} images left in the directory {dir_path}.")

!cp -r /content/pothole-detection-1 /content/orig_pothole_detection_dataset

!ls -l /content/orig_pothole_detection_dataset/train/images | wc -l
!ls -l /content/orig_pothole_detection_dataset/test/images | wc -l
!ls -l /content/orig_pothole_detection_dataset/valid/images | wc -l

!ls -l /content/pothole-detection-1/train/images | wc -l
!ls -l /content/pothole-detection-1/test/images | wc -l
!ls -l /content/pothole-detection-1/valid/images | wc -l

delete_images(list_of_duplicates_train, dataset.location + '/train/images')
delete_images(list_of_duplicates_test, dataset.location + '/test/images')
delete_images(list_of_duplicates_valid, dataset.location + '/valid/images')

!ls -l /content/pothole-detection-1/train/images | wc -l
!ls -l /content/pothole-detection-1/test/images | wc -l
!ls -l /content/pothole-detection-1/valid/images | wc -l

#Delete blurry images for each set
delete_images(list_of_blurry_images_train, dataset.location + '/train/images')
delete_images(list_of_blurry_images_test, dataset.location + '/test/images')
delete_images(list_of_blurry_images_valid, dataset.location + '/valid/images')

workspace = rf.workspace("pothole-h33io")
# workspace.create_project("pothole-cleaned", "object-detection", "MIT", "pothole")
#group (last arg) has to be pothole, not yolov8

from google.colab import files
uploaded = files.upload()

import os
dir = "images/"
project = workspace.project("pothole-cleaned")
for file in os.listdir(dir):
  project.upload(dir + file)
  print(file)

project = workspace.project("pothole-cleaned")
project.upload("truck_pothole.jpg")

pip install requests

import glob
import os
from roboflow import Roboflow
import json

img_dir = "images/"
label_dir = "labels/"
img_file_extension = ".jpg"
label_file_extension = ".json"
annotation_glob = glob.glob(label_dir + '/*' + label_file_extension)

# Get the upload project from Roboflow workspace
project2 = rf.workspace().project("pothole-detection-rzpei")

# Upload images
image_glob = glob.glob(img_dir + '/*' + img_file_extension)
image_glob.sort()
annotation_glob.sort()

# Upload images with and without annotations
#annotations are stored in json format, if predictions is empty, uploading json file will cause single_upload() to fail
for image_path in image_glob:
    annotation_path = label_dir + os.path.splitext(os.path.basename(image_path))[0] + label_file_extension
    if os.path.exists(annotation_path):
        # Check if annotation file has predictions
        with open(annotation_path, 'r') as f:
            annotation_data = json.load(f)
        if annotation_data.get("predictions"):  # Check if 'predictions' key exists and is not empty
            # Upload with annotation
            print(project2.single_upload(
                image_path=image_path,
                annotation_path=annotation_path,
                # ... other parameters for single_upload
            ))
        else:
            print(f"{image_path} has empty predictions.")
            project2.upload(image_path=image_path)
    else:
      print("No annotation file")

from roboflow import Roboflow

# Initialize the Roboflow object with your API key
rf = Roboflow(api_key="CWhevHNkKVaAMb39zuOy")

# Retrieve your current workspace and project name
print(rf.workspace())

# Specify the project for upload
# let's you have a project at https://app.roboflow.com/my-workspace/my-project
workspaceId = 'pothole-h33io'
projectId = 'pothole-detection-rzpei'
project = rf.workspace(workspaceId).project(projectId)

# Upload the image to your project
print(project.upload("images/manchester.jpg"))
print(project.upload("images/evening_pot.jpg"))

workspace.upload_dataset(
    "/content/pothole-cleaned-6", # This is your dataset path
    "pothole-cleaned", # This will either create or get a dataset with the given ID
    num_workers=10,
    project_license="MIT",
    project_type="object-detection",
    batch_name=None,
    num_retries=0
)

"""**Finds all file extensions in each image directory for testing script**"""

!find ./orig_pothole_detection_dataset/train/images/ -type f | perl -ne 'print $1 if m/\.([^.\/]+)$/' | sort -u
!find ./orig_pothole_detection_dataset/valid/images/ -type f | perl -ne 'print $1 if m/\.([^.\/]+)$/' | sort -u
!find ./orig_pothole_detection_dataset/test/images/ -type f | perl -ne 'print $1 if m/\.([^.\/]+)$/' | sort -u

!find ./pothole-detection-1/train/images/ -type f | perl -ne 'print $1 if m/\.([^.\/]+)$/' | sort -u
!find ./pothole-detection-1/valid/images/ -type f | perl -ne 'print $1 if m/\.([^.\/]+)$/' | sort -u
!find ./pothole-detection-1/test/images/ -type f | perl -ne 'print $1 if m/\.([^.\/]+)$/' | sort -u

!nvidia-smi

from ultralytics import YOLO

# Load a model
model = YOLO("yolov8n.pt")  # load a pretrained model (recommended for training)
# Train the model
results = model.train(data=dataset.location+"/data.yaml", epochs=100, imgsz=640)
